{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-8c5707e6-f9a1-471b-af91-ef87f9957461",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "# Milestone 2\n",
    "\n",
    "### Differtless: Teresa Datta, Anastasia Ershova, Mark Penrod, Will Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-0134a7b5-552a-4ec6-ba6b-8974495d6490",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Introduction \n",
    "Differentiation of functions helps find the maxima, minima, and zero points that are essential for optimization and prediction in scientific problems. Automatic differentiation (AD) is a computational approach that automatically finds the derivatives of a function using a computer program. For a given function, the AD software should be able to compute the derivatives of any order with machine precision. \n",
    "\n",
    "Automatic differentiation is different from classical methods such as symbolic differentiation and numerical differentiation, as AD method requires much less computational time and is not affected by numerical instability unlike classical methods.\n",
    "\n",
    "\n",
    "Differtless is a software package for performing AD, with additional functionality for performing optimization to find stationary points of functions.\n",
    "\n",
    "## Background\n",
    "Automatic differentiation is based on the decomposition of differentials using chain rule. Separated by the order of chain rule calculation, AD can be divided into two modes: forward mode and reverse mode. The forward mode calculates derivatives from the inner function to the outer function using chain rule repeatedly, while the reverse mode calculates from outside to inside. \n",
    "We can also say the forward mode calculates the product of the function's Jacobian matrix $J$ with the seed vectors specified by input variables, while reverse modes computes the product of the transpose of Jacobian matrix $J^T$ with  seed vectors. In a function with $m$ inputs and $n$ functions, represented by $R^m \\rightarrow R^n$, forward mode is more efficient when $n$ >> $m$, while reverse mode is more efficient when $n$ << $m$.\n",
    "\n",
    "A computational graph is useful in visualizing the propagation of elementary operations that construct the function to be auto-differentiated. From here, it is easy to evaluate the numerical value after each operation, the derivatives of the elementary operations, and numerical values of derivatives with respect to each input variable.\n",
    "\n",
    "For an example function \n",
    "\n",
    "$$\n",
    "f(x,y)=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "c_1 & c_2\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x  \\\\\n",
    "y\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\n",
    "$$\n",
    "\n",
    "The computational graph is \n",
    "![graph](graph.png)\n",
    "\n",
    "The evaluation trace is \n",
    "![trace](trace.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-e5c04335-2fa1-4f80-9e84-8906086c9097",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## How to use Differtless\n",
    "\n",
    "`differtless` can be obtained by `git clone https://github.com/differtless/cs107-FinalProject.git`\n",
    "\n",
    "The user can then install the `differtless` package by running `pip install .`\n",
    "\n",
    "Installation of package dependencies is handled in `setup.py` \n",
    "\n",
    "    # example installation\n",
    "    >> git clone https://github.com/differtless/cs107-FinalProject.git\n",
    "    >> cd cs107-FinalProject\n",
    "    >> pip install .\n",
    "    \n",
    "\n",
    "\n",
    "Once the user has downloaded differtless they can run `from differtless import ad` and `import differtless.operations as op` in their driver script. Then when they have a function that they would like to automatically differentiate, they will: \n",
    "\n",
    "1. Define the function, using either elementary operations (which we have overloaded- add, subtract, etc) or `differtless` operations for elementary functions (e.g. `op.sin`, `op.exp`)\n",
    "\n",
    "2. User calls the forward function with their pre-defined function, inputs, and seeds.\n",
    "    *  This includes the preprocessing step.\n",
    "\n",
    "This will return the value and corresponding derivatives.\n",
    "\n",
    "##### Example: \n",
    "\n",
    "    from differtless import ad\n",
    "    import differtless.operations as op\n",
    "\n",
    "    # Define a function and provide inputs/seeds\n",
    "\n",
    "    def my_fav_function(x, y, z):\n",
    "        return x*y + op.exp(z)\n",
    "\n",
    "    inputs = [1,2,3]\n",
    "    seeds = [[42, 1, 1], [2, 42, 2], [3, 3, 42]]\n",
    "\n",
    "    # Run automatic differentiation\n",
    "\n",
    "    my_fav_function_ad = ad.forward(my_fav_function, inputs, seeds)\n",
    "\n",
    "    print(f'Function value: {my_fav_function_ad.value}')\n",
    "\n",
    "    print(f'Function derivatives: {my_fav_function_ad.gradients}')\n",
    "\n",
    "    print(f'Function Jacobian: {ad.Jacobian(my_fav_function, inputs)}')\n",
    "\n",
    "    ## Additional optimization routine - not yet implemented\n",
    "\n",
    "    # my_minimum = ad.minimize(func = my_fav_function, guess = [42, 42, 42])\n",
    "\n",
    "    # print(my_minimum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-a0205773-751d-4a87-8ca4-fbe880f2a1b0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Demo: \n",
    "Now you try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-e7fbafa5-8bdc-47b4-a81b-24024e0f5acb",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. Import\n",
    "from differtless import ad\n",
    "import differtless.operations as op\n",
    "\n",
    "#2. Define a function you are interested\n",
    "def function_of_interest(x,y):\n",
    "    #i.e. return x + y\n",
    "    #i.e. return (op.exp(x)^2)/y\n",
    "    pass\n",
    "\n",
    "#3. Define inputs, a list of length N, the number of inputs to your previously defined function\n",
    "inputs = ___\n",
    "\n",
    "#4. Define seeds (an optional step), a list of N lists of length N\n",
    "seeds = ____\n",
    "\n",
    "#5. Run automatic differentiation\n",
    "function_ad = ad.forward(___, ___, ___)\n",
    "\n",
    "print(f'Function value: {function_ad.value}')\n",
    "\n",
    "print(f'Function derivatives: {function_ad.gradients}')\n",
    "\n",
    "print(f'Function Jacobian: {ad.Jacobian(function_of_interest, inputs)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-0bc97f66-1e94-47e1-8772-4908bb23c8e4",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Software Organization\n",
    "\n",
    "Directory Structure:\n",
    "\n",
    "differtless/\n",
    "* README.md\n",
    "* setup.py\n",
    "* .gitignore\n",
    "* .travis.yml\n",
    "* codecov.yml\n",
    "* requirements.txt\n",
    "* docs/\n",
    "    * milestone1.ipynb\n",
    "    * milestone2_progress.md\n",
    "    * milestone2.ipynb *(this document)*\n",
    "    * tutorial.ipynb\n",
    "* tests/\n",
    "    * tests.py\n",
    "* differtless/\n",
    "    * \\_\\_init__.py\n",
    "    * operations.py\n",
    "    * ad.py\n",
    "\n",
    "Functionality: \n",
    "* Documentation will provide both docstring style documentation (in addition to docstrings in .py files) and also interactive short tutorials in the form of jupyter notebooks.\n",
    "* `tests` provides for a comprehensive set of testing programs, and is where our Test Suite will live!\n",
    "* `operations` includes elementary functions such as sin and exp (based on `numpy`), adapted for our datatype to yield their evaluation and derivative\n",
    "* `ad` implements the forward mode of automatic differentiation: preprocessing of user inputs, execution, and future optimization functionality (`scipy` required for optimization)\n",
    "\n",
    "Testing: \n",
    "The test suite will live in tests/ and uses `pytest`. We utilize Travis CI as a continuous integration mechanism to ensure our project is constantly building and passing. We also use CodeCov to ensure our testing suite is testing all parts and branches of the code. \n",
    "\n",
    "We will distribute our package via pip (PyPi) and Conda. Currently it is only available through GitHub (see above for installation instructions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-8b43838e-4242-48d6-9037-d093f2abe160",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "`numpy` for elementary functions, `scipy` for optimization and other functions\n",
    "\n",
    "### Core Data Structures and Classes\n",
    "\n",
    "Our implementation, through our created class `FuncInput` takes advantage of NumPy's `array` for storing FuncInput value and derivative information. NumPy arrays have the advantage of mutability and simple execution of pair-wise vector operations.\n",
    "\n",
    "### Handling Vector-Valued Functions\n",
    "\n",
    "The user can input vectors in order to automatically differentiate vector-valued functions by inputing a tuple or list to represent vector values. During the preprocessing step each of the components will be converted to our `FuncInput` data type.\n",
    "\n",
    "### `operations` module\n",
    "\n",
    "Here we define elementary functions for which we cannot use operator overloading to return their evaluation and derivative (requires `numpy` as a dependency).\n",
    "    * For future milestones, we plan to implement a wider selection of `numpy` and `scipy` methods \n",
    "\n",
    "### `ad` module\n",
    "\n",
    "* `preprocess` function\n",
    "    * Allows us to deal with scalars and vectors\n",
    "    * Takes in a `list`  of `inputs` (1 x N) (where each input can be a scalar, or a tuple or list- for handling vector values) and an optional matrix `seeds` (N x N)\n",
    "    * If the user inputs a scalar it will be converted to a 1 x 1 vector\n",
    "    * `seeds` defaults to None, in which case we will use an N x N identity matrix where `N = len(inputs)`\n",
    "    * For each value in `inputs` (and if the inputs are vectors, each component in each vector) and row in `seeds` we will instantiate the `FuncInput` object described below\n",
    "* `FuncInput` class\n",
    "    * Parameters: `input` (value - `np.array`) and `seed` (seed vector – `np.array`) \n",
    "    * The class has two instance variables: `self.val_ = value` (value) and `self.ders_ = seed` (list of derivative values)\n",
    "        * Two getter methods are implemented with the `@property` decorator to allow indirect access to the instance variables\n",
    "    * In this class we use dunder methods for operator overloading of elementary operations (e.g. `__add__, __sub__, __mul__, __floordiv__, __truediv__, __mod__, __pow__`) to return evaluation and derivative\n",
    "    * In our `Operations` module we implement other elementary operations (e.g. sin, exp)\n",
    "    * In both cases the methods return a new instance of `FuncInput` with the `evaluation` and `[evaluation of derivatives]` as inputs\n",
    "* `forward` function\n",
    "    * Parameters: `func` (user-defined function, can be scalar or vector), `inputs`, `seeds`\n",
    "    * Executes `preprocess` function on `inputs` and `seeds` to convert to FuncInput objects\n",
    "    * Executes forward mode of automatic differentiation\n",
    "    * Returns evaluation and derivative calculation\n",
    "* `Jacobian` function\n",
    "    * Parameters: `func` (user-defined function, can be scalar or vector), `inputs`\n",
    "    * Executes `forward` function on the function, `func`, and `inputs` using the default `seed` (`[]` which will produce an indentity matrix)\n",
    "    * Accesses the gradient of the result, which is the Jacobian\n",
    "* `minimize` function\n",
    "    * not yet implemented, but planned for future work as outlined below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-fd4a71cc-fcb2-4deb-89d8-893a277c52cc",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Future Features\n",
    "\n",
    "Given that a large application space for AD is optimization, we plan to integrate `scipy`'s optimization routines to work directly with `differtless`:\n",
    "\n",
    "* `minimize` function\n",
    "    * Parameters: `func` (user-defined function), `guess` (initial guess), `scipy.optimize.minimize` `*args`\n",
    "    * Uses forward functionality to calculate Jacobian matrix and interfaces with `scipy.optimize.minimize`\n",
    "    * Returns optimization result\n",
    "\n",
    "In order to extend the functionality of `differtless` beyond what is currently possible with conventional AD packages (such as `autograd`), we also plan to:\n",
    "\n",
    "* first implement a wider range of `numpy` and `scipy` functions than the elemental functions to make the utility of our package more comparable\n",
    "* implement functions that are not typically implemented in other AD packages, such as:\n",
    "    * `cdf` and `logcdf` methods for distributions like `scipy.stats.multivariate_normal` and `scipy.stats.gamma` – this would be useful for applications such as samplers relying on gradient descent which commonly use automatic differentiation (currently one would need to define the derivatives by hand for distributions not implemented in `autograd`)\n",
    "    * other `scipy` functions like distance metrics (e.g. Hamming distance etc) – implementing these functions directly in `differtless` would avoid the end-user having to manually redefine these.\n",
    "\n",
    "Implementing these features should not require any changes to the directory structure/core classes, as the optimization routines will be implemented as new functions that utilise the core functionality, and any new operations will be implemented in a similar manner to those already in `operations.py`. The main challenge we foresee with these new features is that the implementation of derivatives of functions not commonly available might be challenging, as that might be part of the reason why they are not commonly available."
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a2154f10-d210-402d-9d71-1922a3c9b450",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
