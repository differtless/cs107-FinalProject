{"cells":[{"cell_type":"markdown","source":"# Differtless Documentation\n### CS 107 Group 33: Teresa Datta, Anastasia Ershova, Mark Penrod, Will Wang","metadata":{"tags":[],"output_cleared":false,"cell_id":"00000-b7c82bc6-c8ef-4272-8013-f8a42ecd0842","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Introduction \nDifferentiation of functions helps find the maxima, minima, and zero points that are essential for optimization and prediction in a variety of problems across a wide range of fields (i.e. Machine Learning, Physics, Fluid Mechanics, and more). Automatic differentiation (AD) is a computational approach that automatically finds the derivatives of a function using a computer program. For a given function, the AD software should be able to compute the derivatives of any order with machine precision. Through the computation of the Jacobian matrix and gradients, we are also able to optimize. \n\nAutomatic differentiation differs from classical methods such as symbolic differentiation and numerical differentiation in that the AD method requires much less computational time and is not affected by numerical instability.\n\n\nDiffertless is a software package for performing AD, with additional functionality for performing optimization to find stationary points of functions. Our goal is to make automatic differentiation effortless!\n\n## Background\nAutomatic differentiation is based on the decomposition of differentials using chain rule. Separated by the order of chain rule calculation, AD can be divided into two modes: forward mode and reverse mode. The forward mode calculates derivatives from the inner function to the outer function using chain rule repeatedly, while the reverse mode calculates from outside to inside. \nWe can also say the forward mode calculates the product of the function's Jacobian matrix $J$ with the seed vectors specified by input variables, while reverse modes computes the product of the transpose of Jacobian matrix $J^T$ with  seed vectors. In a function with $m$ inputs and $n$ functions, represented by $R^m \\rightarrow R^n$, forward mode is more efficient when $n$ >> $m$, while reverse mode is more efficient when $n$ << $m$.\n\nFor our extension, we implemented optimization functionality for `numpy` and `scipy` mathematical operations, probability distributions, and spatial functions. `numpy` and `scipy` are popular open-source libraries for Python featuring a large collection of high-level numerical computing tools and modules for optimization, linear algebra, integration, interpolation, and special functions for tasks common in science and engineering.\nThere are many useful applications of AD in this realm: for example, AD is often used within the context of Bayesian parameter inference for sampling from the posterior using methods like Hamiltonian Monte Carlo, where one needs to be able to take the gradients of probability distributions. \n\nThus, we implemented the probability and cumulative distribution functions of commonly used statistical distributions such as the Normal, Poisson and Gamma distributions. When performing parameter inference in this manner, users often work with the logarithms of the functions to prevent overflow or underflow errors â€“ thus we also implemented the log PDFs and log CDFs of all these distributions. We note that a lot of this functionality is not available in other AD packages like autograd.\n\nWe hope that differtless will be a convenient way for users in a range of disciplines to integrate AD into their already-established workflows, without having to redefine functions they would typically use packages like numpy and scipy for, and without having to modify their pipeline to incorporate optimization routines.\n\nA computational graph is useful in visualizing the propagation of elementary operations that construct the function to be auto-differentiated. From here, it is easy to evaluate the numerical value after each operation, the derivatives of the elementary operations, and numerical values of derivatives with respect to each input variable.\n\nFor an example function \n\n$$\nf(x,y)=\n\\left[\n\\begin{matrix}\nc_1 & c_2\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nw_{11} & w_{12} \\\\\nw_{21} & w_{22}\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nx  \\\\\ny\n\\end{matrix}\n\\right]\n\n$$\n\nThe computational graph is \n![computational_graph](comp.png)\n\nThe evaluation trace is \n![trace](trace.png)\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00001-9e09f66f-274d-4af8-8777-9096597bdc12","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## How to use Differtless\n\nDiffertless is available from PyPI via `pip install differtless`\n\n`differtless` can also be obtained by `git clone https://github.com/differtless/cs107-FinalProject.git`\n\nIf a user obtained `differtless` from github, they can install the package by running `pip install .`\n\nIn both cases, package requirements are provided and checked for via `setup.py` \n\n    # example github installation\n    >> git clone https://github.com/differtless/cs107-FinalProject.git\n    >> cd cs107-FinalProject\n    >> pip install .\n    \n\nOnce the user has downloaded differtless they can run `from differtless import ad` and `import differtless.operations as op` in their driver script. Then when they have a function that they would like to automatically differentiate, they will: \n\n1. Define the function, using either elementary operations (which we have overloaded- add, subtract, etc) or `differtless` operations for elementary functions (e.g. `op.sin`, `op.exp`)\n    * The user may also define multiple functions (NOTE: all functions must have the same parameters, regardless of whether they are used in the body of the function)\n2. User defines inputs and optional seed\n    * Scalar inputs: \n        * Single scalars may be in the form of a number or `list` containing one number\n        * Multiple scalars must be input as a `list` or `tuple`\n    * Vector inputs:\n        * Single and multiple vectors must both be defined as a list of lists or tuple of tuples\n        * Ex. `x = [[1,2,3]]`\n    * Seed must be in `list` form (or list of lists)\n3. User calls the forward function with their pre-defined function (or list of functions), inputs, and seeds.\n    * This includes the preprocessing step.\n    * NOTE: the seed matrix is optional, if no seed is specified the seed will be the appropriate identity matrix\n    * This will return the value and corresponding derivatives.\n4. User can also call `ad.minimize`, `ad.root`, or `ad.least_squares` to utilize optimization functionality \n    * users can also call on probability distributions (such as Normal, Poisson, Gaussian)  and the special functions overloaded in the `operations` module\n\n\n##### Example: \n\n    from differtless import ad\n    import differtless.operations as op\n\n    # Define a function and provide inputs/seeds\n\n    def f1(x, y, z):\n        return x*y + op.exp(z)\n\n    def f2(x, y , z):\n        return (op.exp(x/y) + z)/y\n\n    # scalar inputs\n    inputs = [1,2,3]\n    seeds = [[42, 1, 1], [2, 42, 2], [3, 3, 42]]\n\n    # vector inputs\n    inputs = [[3, 2, 1], [4, 5, 6]]\n    seeds = [[1, 0], [0, 1]]\n\n    # Run automatic differentiation\n    f1_ad = ad.forward(f1, inputs, seeds)\n\n    # Run AD with vector-valued function\n    f1_f2_ad = ad.forward([f1, f2], inputs, seeds)\n\n    # minimizing solution\n    min_sol = ad.minimize(f1, [2, 3, 4])\n\n    print(f'Function value: {f1_ad.value}')\n    print(f'Function derivatives: {f1_ad.gradients}')\n    print(f'Function Jacobian: {ad.Jacobian(f1, inputs)}')\n    print(f'Optimization minima solution: {min_sol}')\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00002-278d3f27-5be6-432e-9a61-b6ccde478406","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"#### Full Functionality Demo: also available as demo.ipynb\n","metadata":{"tags":[],"cell_id":"00003-c290ded2-471b-408a-8504-a0d3f4559bde","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-01c9a38a-ffe4-4049-93f2-bd0b83d1e038","output_cleared":true,"source_hash":null,"execution_start":1607713345250,"execution_millis":75,"deepnote_cell_type":"code"},"source":"### import differtless ###\nfrom differtless import ad\nimport differtless.operations as op\n\n### define inputs of interest ###\n# scalar input\nsingle_scalar = [5]\n# multiple scalar inputs (include seed)\nmulti_scalar = [2,3,4]\n# vector input\nsingle_vector = [[4,5,6]]\n# multiple vector inputs\nmulti_vector = [[1,2,3],[4,5,6],[7,8,9]]\n# seed matrix for multivariable functions\nmulti_seed = [[42,1,1],[2,42,1],[3,3,42]]\n\n### define functions of interest ###\n# single-variable functions\ndef single_f1(x):\n    return op.exp(op.sin(x) ** 2)\ndef single_f2(x):\n    return op.log(2 / x) - x\n# multivariable functions\ndef multi_f1(x, y, z):\n    return (z ** (x - op.tanh(x + y)))/x\ndef multi_f2(x, y, z):\n    return x ** (10 - y)","execution_count":1,"outputs":[]},{"cell_type":"code","source":"### Preprocessing example ###\nad.preprocess(multi_vector)","metadata":{"tags":[],"cell_id":"00006-08de6a7f-0272-416a-a830-98f22caf8aab","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Scalar-Valued Functions ###\n# Scalar input, single variable\nforward1 = ad.forward(single_f1, single_scalar)\njacobian1 = ad.Jacobian(single_f1, single_scalar)\nprint(f'Value = {forward1.value}\\nGradient = {forward1.gradients}\\nJacobian = {jacobian1}')\n\n# Vector input, single variable\nforward2 = ad.forward(single_f1, single_vector)\njacobian2 = ad.Jacobian(single_f1, single_vector)\nprint(f'Value = {forward2.value}\\nGradient = {forward2.gradients}\\nJacobian = {jacobian2}')\n\n# Scalar input, multivariable\nforward3 = ad.forward(multi_f1, multi_scalar, multi_seed)\njacobian3 = ad.Jacobian(multi_f1, multi_scalar)\nprint(f'Value = {forward3.value}\\nGradient = {forward3.gradients}\\nJacobian = {jacobian3}')\n\n# Vector input, multivariable\nforward4 = ad.forward(multi_f1, multi_vector)\njacobian4 = ad.Jacobian(multi_f1, multi_vector)\nprint(f'Value = {forward4.value}\\nGradient =\\n{forward4.gradients}\\nJacobian =\\n{jacobian4}')","metadata":{"tags":[],"cell_id":"00006-80ae2a6e-fba7-49b2-b766-44f33ff42b7c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Vector-Valued Functions ###\n# Scalar input, single variable\nforward5 = ad.forward([single_f1, single_f2], single_scalar)\njacobian5 = ad.Jacobian([single_f1, single_f2], single_scalar)\nprint(f'Value = {forward5.value}\\nGradient = {forward5.gradients}\\nJacobian = {jacobian5}')\n\n# Scalar input, multivariable\nforward6 = ad.forward([multi_f1, multi_f2], multi_scalar, multi_seed)\njacobian6 = ad.Jacobian([multi_f1, multi_f2], multi_scalar)\nprint(f'Value = {forward6.value}\\nGradient =\\n{forward6.gradients}\\nJacobian =\\n{jacobian6}')\n\n# Vector input, single variable\nforward7 = ad.forward(multi_f1, multi_vector, multi_seed)\njacobian7 = ad.Jacobian(multi_f1, multi_vector)\nprint(f'Value = {forward7.value}\\nGradient =\\n{forward7.gradients}\\nJacobian =\\n{jacobian7}')","metadata":{"tags":[],"cell_id":"00007-e730e906-27e2-45c2-8777-a8c625b27702","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Extra Feature- Optimization ###\n\n# Function optimization\nad.minimize(multi_f2, [2, 3, 4])\n\n# Least-squares solution with bounds on the variables\nad.least_squares(single_f1, 4, bounds=(-4, 10))\n\n# Root Finding\nad.root(single_f2, 1)","metadata":{"tags":[],"cell_id":"00008-7b79c1de-bf9f-4f26-9edb-08dd3c03dad5","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Extra Feature - Probability Distributions ###\n\n# Log CDF of Normal distribution\nforward8 = ad.forward(op.Normal(loc=3, scale=2).logcdf, single_vector)\njacobian8 = ad.Jacobian(op.Normal(loc=3, scale=2).logcdf, single_vector)\nprint(f'Value = {forward8.value}\\nGradient =\\n{forward8.gradients}\\nJacobian =\\n{jacobian8}')\n\n# Joint PDF of multiple variables described by different probability distributions\ndef joint_probabilities(x, y, z):\n    return op.Normal(loc=1, scale=1).pdf(x) * op.Gamma(alpha=1, beta=2).pdf(y) * op.Poisson(mu=3).pmf(z)\nforward9 = ad.forward(joint_probabilities, multi_vector, multi_seed)\nprint(f'Value = {forward9.value}\\nGradient =\\n{forward9.gradients}')\n\n# Special Functions\ndef special(x):\n    return 2*op.gamma(x) - op.erf(x/3)\nforward10 = ad.forward(special, single_vector)\nprint(f'Value = {forward10.value}\\nGradient =\\n{forward10.gradients}')","metadata":{"tags":[],"cell_id":"00009-36a45433-98ff-4380-a520-f176f80ed4e9","output_cleared":true,"source_hash":null,"execution_start":1607714160946,"execution_millis":0,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Software Organization\n\nDirectory Structure:\n\ndiffertless/\n* README.md\n* setup.py\n* .gitignore\n* .travis.yml\n* codecov.yml\n* requirements.txt\n* docs/\n    * documentation.ipynb *(this document)*\n    * demo.ipynb\n    * milestone1.ipynb\n    * milestone2_progress.md\n    * milestone2.ipynb\n* tests/\n    * tests.py\n* differtless/\n    * \\__init__.py\n    * operations.py\n    * ad.py\n\nFunctionality: \n* Documentation provides detailed desciption of our system via documents such as this one which you are reading. Our documentation also includes docstring style documentation in .py files and an interactive short tutorials in the form of a `demo.ipynb` jupyter notebooks.\n* Tests provides for a comprehensive set of testing programs implemented via PyTest, and is where our Test Suite will live!\n* Operations includes basic operations (addition, subtraction, power, etc), comparison operators, elementary functions, and numpy and scipy operators and probability distributions adapted for our `FuncInput` datatype to yield their evaluation and derivative.\n* `ad.py` implements the forward mode of automatic differentiation: preprocessing of user inputs, execution, and extra optimization functionality (`scipy` required for optimization)\n\nTesting: \nThe test suite will live in `tests/` and uses `pytest`. We utilize Travis CI as a continuous integration mechanism to ensure our project is constantly building and passing. We also use CodeCov to ensure our testing suite is testing all parts and branches of the code. \n\nWe distribute our package via pip (PyPi).","metadata":{"tags":[],"output_cleared":false,"cell_id":"00005-9baeb313-17cb-482a-b5ad-ae0e26599484","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Implementation\n\n\n### Dependencies\n\n`numpy` for elementary functions, `scipy` for optimization and other functions\n\n### Core Data Structures and Classes\n\nOur implementation, through our created class `FuncInput` takes advantage of NumPy's `array` for storing FuncInput value and derivative information. NumPy arrays have the advantage of mutability and simple execution of pair-wise vector operations.\n\n### Handling Vector-Valued Functions\n\nFor vector-valued functions, the user will define each of the desired functions then pass them in as a list into the `forward` or `Jacobian` functions. \n\n### `operations` module\n\nHere we define elementary functions for which we cannot use operator overloading to return their evaluation and derivative (requires `numpy` as a dependency).\n* We also implemented a wide selection of `numpy` and `scipy` optimization, spacial and mathematical methods as described above and in the Additional Features section\n\n### `ad` module\n\n* `preprocess` function\n    * Allows us to deal with scalars and vectors\n    * Takes in a `list`  of `inputs` (1 x N) (where each input can be a scalar, or a tuple or list- for handling vector values) and an optional matrix `seeds` (N x N)\n    * If the user inputs a scalar it will be converted to a 1 x 1 vector\n    * `seeds` defaults to None, in which case we will use an N x N identity matrix where `N = len(inputs)`\n    * For each value in `inputs` and row in `seeds` we will instantiate the `FuncInput` object described below\n* `FuncInput` class\n    * Parameters: `input` (value - `np.array`) and `seed` (seed vector â€“ `np.array`) \n    * The class has two instance variables: `self.val_ = value` (value) and `self.ders_ = seed` (list of derivative values)\n        * Two getter methods (`value` and `gradients`) are implemented with the `@property` decorator to allow indirect access to the instance variables\n    * In this class we use dunder methods for operator overloading of elementary operations (e.g. `__add__`, `__sub__`, `__mul__`, `__truediv__`, `__pow__`) to return evaluation and derivative\n    * In our `operations` module we implement other elementary operations (e.g. sin, exp)\n    * In both cases the methods return a new instance of `FuncInput` with the `evaluation` and `[evaluation of derivatives]` as inputs\n* `forward` function\n    * Parameters: `func` (user-defined function(s)), `inputs`, `seeds`\n    * Executes `preprocess` function on `inputs` and `seeds` to convert to FuncInput objects\n    * Executes forward mode of automatic differentiation\n    * Returns evaluation and derivative calculation\n* `Jacobian` function\n    * Parameters: `func` (user-defined function(s)), `inputs`\n    * Executes `forward` function on each function, `func`, and `inputs` using the default `seed` (`[]` which will produce an indentity matrix)\n    * Returns the gradient of the result, which is the Jacobian\n* `minimize` function\n    * scipy.optimize.minimize wrapper\n    * Parameters: `fun` (user-defined function), `x0` (ndarray, shape (n, ): initial guess), `discriptive`= False (whether to return full scipy `OptimizeResult`),`scipy.optimize.minimize *args`\n    * Makes `fun` compatible with scipy and uses differtless to calculate Jacobian, then performs optimization using `scipy.optimize.minimize`\n    * Returns solution array or `OptimizeResult` scipy object\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-9f2ce178-083e-46bd-8c95-a21a74eb5e92","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Additional Features\n\nGiven that a large application space for AD is optimization, we built upon the utility of numpy and scipy and implemented two main features for our extension:\n\n1.  Wrappers for scipyâ€™s optimization routines (such as scipy.optimize.minimize, root, and least_squares)\n    - These automatically feed in jacobians calculated by differtless into the optimizer, and the user can simply input their multi-argument functions as is into our functions, without having to convert them to a scipy-compatible format (in which the function can only take a single argument that is a list of the parameters).\n    - `minimize`: scipy.optimize.minimize wrapper\n        - Parameters: `fun` (user-defined function), `x0` (ndarray, shape (n, ): initial guess), `discriptive = False` (whether to return full scipy `OptimizeResult`),`scipy.optimize.minimize *args`\n        - Makes `fun` compatible with scipy and uses differtless to calculate Jacobian, then performs optimization using `scipy.optimize.minimize`\n        - Returns solution array or `OptimizeResult` scipy object\n\n\n2.  Implementations for scipy functions, special functions, spatial functions, statistical distributions, and a majority of the numpy library mathematical operations\n    - With this practicality, the user does not need to redefine their functions to compute the gradient.\n    - We particularly focused on common use-cases for AD and functions that are not available in other AD packages. \n    - This includes probability distributions such as Poisson, Gamma, and Normal distributions and their log PDFs and log CDFs. \n        - These distributions were implemented as classes, so users can create instances of these distributions, setting the appropriate parameters for each distribution (i.e. Normal` object with mean and standard deviation), and then utilize the pdf/ logpdf/ cdf/ logcdf class methods\n    - This also includes special functions (error function, gamma function, factorial, floor, and lower incomplete gamma function)\n    - We also implemented spatial functions like Euclidean distance that are also not available in other AD packages\n    - If users wants to implement their own operations, they can use the `validate_input` wrapper to ensure compatibility for single-input functions and `validate_input_multiple` for functions that take additional arguments \n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00007-167735ef-f277-4e47-be97-eae34bdd8ec4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Inclusion Statement \nDiffertless is committed to developing a culture of inclusion. Only by learning from a diverse community of contributors are we able to build the best product and grow to be our best selves. We welcome all educational levels, cultures, ethnicities, races, gender identities and expressions, nations of origin, ages, languages spoken, veteranâ€™s status, colors, religions, disabilities, geographic backgrounds, sexual orientations and beliefs.\n\n*We embrace your unique background, because you belong here.*\n\n## Broader Impact and Inclusivity Statement\n\nOne of the key applications of automatic differentiation is for optimization, and we specifically explored the implementations and utility of minimization via scipy.optimize.minimize. Although the technical details for how the minima of these functions may feel far removed from any potential negative intent, it is vital to remove our rose-colored glasses and remember that technology is never created in a void. \n\nImagine, for example a machine learning algorithm for which we have determined a loss function- the most obvious way to get the best model would then be to minimize the loss function, potentially via our AD package. That's in large part because as computer scientists, our understandings of what makes a model the \"best\" focus accuracy metrics as optimized via a loss function. However, for models with real-world impact, they need to be both **accurate and fair**. These are two separate objectives with different metrics of success- accuracy may be measured by our loss function, but fairness requires a deeper dive:  understanding the pre-existing biases in our dataset, comparing the accuracy/false positive/false negative rates between any protected groups. \nWhen accuracy is too heavily optimized, the real-world impacts can be grave. The effects of algorithmic bias have caused it to be harder for certain minority groups to be hired under the same qualifications (2), and in the contexts of facial recognition systems, at the expense of ethical introspection, these algorithms are known to have lower accuracy on people of color and have caused the unnecessary and traumatic arrests of innocent men of color in the U.S. (4) and the tracking and controlling of the Uighur minority group in China (1).\n\nHowever, there are still strong motivations for pursuing this type of work and potential positive impacts. Recently, in academic fields such as flow topology (5) and aerodynamic design frameworks (6), automatic differentiation have been used to advance the understanding of necessary and previouisly less understood problems.\n\nMarginalized groups are still severely underrepresented in tech, and this disparity is even more apparent in the open source community. \nFor example, a Toptal study (7) found that in their random sample, just 5.4% of GitHub users with >10 contributions are female.\nTo this end, we've attempted to address this subtle barriers in order to make our repo as welcoming and inclusive as possible. The first step we took to make clear that all backgrounds belong here in our contributing community was to develop an Inclusion Statement to reflect our commitment to fostering a welcoming community. This is one of the first things that users will see in our repo. \n\nFor our software project, our code contribution is as transparent as possible. Pull requests can be approved and reviewed by any member of our team, and if there is any concern about a PR being approved, a group discussion is initiated to ensure proper vetting and open communication is maintained. \nIt is also worth mentioning that our (randomly assigned) team brings a diverse array of perspectives- both from an internal diversity viewpoint (gender, ethnicity, age, etc.) but also from an academic background viewpoint. We represent a variety of fields and a range of programs under the Harvard umbrella.\nThis is important to note since as Anna-Chiara suggests in the Toptal study, projects that show diversity in their leadership promote a culture of inclusivity within its contributors. \nWe recognize, however, that there are still barriers to certain groups that we have not yet accounted for. One of these is how non-native English speakers will interact with our code. Our entire documentation and examples are written in English, and so if given more time, we would want to make sure there is access to accurately translated versions of our documentation (or at least that it is Google Translate friendly).\n\n\n\n1. https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html\n2. https://www.vox.com/recode/2020/1/1/21043000/artificial-intelligence-job-applications-illinios-video-interivew-act\n3. https://www.theverge.com/2019/11/11/20958953/apple-credit-card-gender-discrimination-algorithms-black-box-investigation\n4. https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html\n5. https://link.springer.com/article/10.1007/s00158-017-1708-2\n6. https://www.ercoftac.org/downloads/bulletin-docs/ercoftac_bulletin_102.pdf \n7. https://www.toptal.com/open-source/is-open-source-open-to-women ","metadata":{"tags":[],"cell_id":"00008-ad74c306-724f-4fcc-ab4e-e32b0cbf3d37","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Future Work\n\nAs a future direction for this work, there a few key areas we would like to expand on. \n\nOne logical next step would be to implement reverse mode. Unlike forward mode which is calculate from the inside out, reverse mode calculates from the outside in and would compute the transpose of the Jacobian matrix. We would also like to expand our suite of vailable functionality to cover an even wider range of statistical distributions such as Multivariate Normal, Laplace, and more. \n\nIn continuing with our goal of making automatic differentiation effortless, we could also craft an educational tool by using our implementation and crafting a user-friendly interface to help the public better understand the utility and inner workings of AD. We also could extend our optimization functionality by implementing other schemes that use Jacobians for optimization such as root finding for vector functions. Finally, we could help remove barriers for non-native english speaking contributors by ensuring there is access to accurately translated or Google Translate friendly versions of our documentation. \n\nWe thoroughly enjoyed making Automatic Differentiation effortless, and we hope you enjoyed learning about our project! Thank you!\n","metadata":{"tags":[],"cell_id":"00009-597a4fa6-c884-418b-8162-c97d6a8b0180","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00010-7c111629-185d-4130-a202-c3d73da85f00","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"a8be6f27-819f-4df8-8cf5-c27c5a4d329b","deepnote_execution_queue":[]}}