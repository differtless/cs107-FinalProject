{"cells":[{"cell_type":"markdown","source":"# Milestone 1\n\n### Differtless: Teresa Datta, Anastasia Ershova, Mark Penrod, Will Wang","metadata":{"tags":[],"cell_id":"00000-231dd2a0-03c5-4ad0-abbb-0247c2388815"}},{"cell_type":"markdown","source":"## Introduction \nDifferentiation of functions helps find the maxima, minima, and zero points that are essential for optimization and prediction in scientific problems. Automatic differentiation (AD) is a computational approach that automatically finds the derivatives of a function using a computer program. For a given function, the AD software should be able to compute the derivatives of any order with machine precision. \n\nAutomatic differentiation is different from classical methods such as symbolic differentiation and numerical differentiation, as AD method requires much less computational time and is not affected by numerical instability unlike classical methods.\n\n\nDiffertless is a software package for performing AD, with additional functionality for performing optimization to find stationary points of functions.\n\n## Background\nAutomatic differentiation is based on the decomposition of differentials using chain rule. Separated by the order of chain rule calculation, AD can be divided into two modes: forward mode and reverse mode. The forward mode calculates derivatives from the inner function to the outer function using chain rule repeatedly, while the reverse mode calculates from outside to inside. \nWe can also say the forward mode calculates the product of the function's Jacobian matrix $J$ with the seed vectors specified by input variables, while reverse modes computes the product of the transpose of Jacobian matrix $J^T$ with  seed vectors. In a function with $m$ inputs and $n$ functions, represented by $R^m \\rightarrow R^n$, forward mode is more efficient when $n$ >> $m$, while reverse mode is more efficient when $n$ << $m$.\n\nA computational graph is useful in visualizing the propagation of elementary operations that construct the function to be auto-differentiated. From here, it is easy to evaluate the numerical value after each operation, the derivatives of the elementary operations, and numerical values of derivatives with respect to each input variable.\n\nFor an example function \n\n$$\nf(x,y)=\n\\left[\n\\begin{matrix}\nc_1 & c_2\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nw_{11} & w_{12} \\\\\nw_{21} & w_{22}\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nx  \\\\\ny\n\\end{matrix}\n\\right]\n\n$$\n\nThe computational graph is \n<img src=\"comp.png\">\n\nThe evaluation trace is \n<img src=\"trace.png\">\n","metadata":{"tags":[],"cell_id":"00001-e7fefa8f-1cdd-4bba-b008-2be6ffc37a0a"}},{"cell_type":"markdown","source":"## How to use Differtless\n\nOur user will import `differtless`. Then when they have a function that they would like to automatically differentiate, they will: \n\n1. Define the function, using either elementary operations (which we have overloaded- add, subtract, etc) or `differtless` operations for elementary functions (e.g. `differtless.operations.sin`, `differtless.operations.exp`)\n\n2. User will preprocess their function inputs to create `differtless.ad.FuncInput` objects (and define their own seed matrix if they choose to, otherwise will default to the identity matrix)\n\n3. Call the forward function with their pre-defined function and inputs\n\nThis will return the value and corresponding derivatives.\n\nFor example:\n\n    import numpy as np\n    import differtless as dfl\n\n    def my_fav_function(x, y, z):\n        return x*y + dfl.operations.exp(z)\n\n\n    inputs = [1,2,3]\n    #for vector values: inputs = [(1, 2), (3, 4), (5, 6)]\n    seeds = np.array([[42, 1, 1], [2, 42, 2], [3, 3, 42]])\n\n    proc_inputs = dfl.ad.preprocess(inputs, seeds)\n    # e.g. proc_inputs = [FuncInput(1, [42, 1, 1]), FuncInput(2, [2, 42, 2]), FuncInput(3, [3, 3, 42])] \n   \n    evaluation, derivatives = dfl.ad.forward(my_fav_function, my_inputs)\n\n    print(evaluation)\n\n    print(derivatives)\n\n    # additional optimization routine\n\n    my_minimum = dfl.ad.minimize(func = my_fav_function, guess = [42, 42, 42])\n\n    print(my_minimum)\n","metadata":{"tags":[],"cell_id":"00002-8a54743e-8d48-425a-ae4d-446354c8301e"}},{"cell_type":"markdown","source":"## Software Organization\n\nDirectory Structure:\n\ndiffertless/\n* README.md\n* setup.py\n* .gitignore\n* .travis.yml\n* docs/\n    * milestone1.ipynb *(this document)*\n    * tutorial.ipynb\n* tests/\n    * tests.py\n* differtless/\n    * operations.py\n    * ad.py\n\nFunctionality: \n* Documentation will provide both docstring style documentation (in addition to docstrings in .py files) and also interactive short tutorials in the form of jupyter notebooks.\n* Tests provides for a comprehensive set of testing programs, and is where our Test Suite will live!\n* Operations includes elementary functions such as sin and exp (based on `numpy`), adapted for our datatype to yield their evaluation and derivative\n* Forward implements the forward mode of automatic differentiation: preprocessing of user inputs, execution, and optimization functionality (`scipy` required for optimization)\n\nTesting: \nThe test suite will live in tests/. We will utilize Travis CI as a continuous integration mechanism to ensure our project is constantly building and passing. We will also use CodeCov to ensure our testing suite is testing all parts and branches of the code. \n\nWe will distribute our package via pip (PyPi) and Conda.\n\nWe currently are not intending to utilize a framework for this project.","metadata":{"tags":[],"cell_id":"00002-90e2d06a-4452-4099-85f2-dc41a35d4aed"}},{"cell_type":"markdown","source":"## Implementation\n\n\n### Dependencies\n\n`numpy` for elementary functions, `scipy` for optimization\n\n### Core Data Structures\n\nOur implementation, through class `FuncInput` takes advantage of a `list` for storing derivative information. The `list` structure allows us to store multiple values with low overhead. A `tuple` could also be used, but the immutability of `tuple`s could cause unforseen issues later on.\n\n### Handling Vector-Valued Functions\n\nThe user can input vectors in order to automatically differentiate vector-valued functions by inputing a tuple or list to represent vector values. During the preprocessing step each of the components will be converted to our `FuncInput` data type.\n\n### `operations` module\n\nHere we define elementary functions for which we cannot use operator overloading to return their evaluation and derivative (requires `numpy` as a dependency).\n\n### `ad` module\n\n* `preprocess` function\n    * Allows us to deal with scalars and vectors\n    * Takes in a `list` or `numpy.array` of `inputs` (1 x N) and an optional matrix `seeds` (N x N)\n    * If the user inputs a scalar it will be converted to a 1 x 1 vector\n    * `seeds` defaults to None, in which case we will use an N x N identity matrix where `N = len(inputs)`\n    * For each value in `inputs` (and if the inputs are vectors, each component in each vector) and row in `seeds` we will instantiate the `FuncInput` object described below\n* `FuncInput` class\n    * Parameters: `input` (value – `float` or `int`) and `seed` (seed vector – `list` or `numpy.array`) \n    * The class will have two instance variables: `self.val = value` (value) and `self.der_val = seed` (list of derivative values)\n    * In this class we will use dunder methods for operator overloading of elementary operations (e.g. `__add__, __sub__, __mul__, __floordiv__, __truediv__, __mod__, __pow__`) to return evaluation and derivative\n    * In our `Operations` module we will implement other elementary operations (e.g. sin, exp)\n    * In both cases the methods will return a new instance of `FuncInput` with the `evaluation` and `[evaluation of derivatives]` as inputs\n* `forward` function\n    * Parameters: `func` (user-defined function, can be scalar or vector), `inputs` (output of preprocessing)\n    * Executes function\n    * Returns evaluation and derivative calculation\n* `minimize` function\n    * Parameters: `func` (user-defined function), `guess` (initial guess), `scipy.optimize.minimize` `*args`\n    * Uses forward functionality to calculate Jacobian matrix and interfaces with `scipy.optimize.minimize`\n    * Returns optimization result","metadata":{"tags":[],"cell_id":"00001-bfb6419b-ad71-4a42-991c-a7a28a2f7e30"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00005-b23a29dc-8533-4a7c-9e1c-2cb871385630"}},{"cell_type":"markdown","source":"## M1 feedback\n\n2/2 Introduction:\nNice job!\n\n1.5/2 Background\nGood start to the background.  The flow could have been enhanced by presenting the evaluation trace and a computational graph.\n\n### Things we add: \n\nFor an example function \n\n$$\nf(x,y)=\n\\left[\n\\begin{matrix}\nc_1 & c_2\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nw_{11} & w_{12} \\\\\nw_{21} & w_{22}\n\\end{matrix}\n\\right]\n\\left[\n\\begin{matrix}\nx  \\\\\ny\n\\end{matrix}\n\\right]\n\n$$\n\nThe computational graph is \n<img src=\"comp.png\">\n\nThe evaluation trace is \n<img src=\"trace.png\">\n3/3 How to use\nIt would be nice to include the installation process.\n\n3/3 Software Organization\nNicely done!\n\n4.5/5 Implementation\n\n1. What are the core data structures and why do use pick these data structures?\n2.  Your implementation for vector-valued functions is unclear.\n\n14/15","metadata":{"tags":[],"cell_id":"00006-1fd165c4-5520-47f9-8f16-a57ddf941145"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"dd8aece9-fac7-4634-9046-f131bf2101a3","deepnote_execution_queue":[]}}